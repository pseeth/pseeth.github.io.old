<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Cover song identification with 2D Fourier transform sequences &middot; Prem Seetharaman
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <script src="/public/js/picturefill.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/wavesurfer.js/1.0.52/wavesurfer.min.js"></script>
  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>Prem Seetharaman</h1>
      <p class="lead">I am a PhD candidate at Northwestern University in the <a href="http://music.cs.northwestern.edu">Interactive Audio Lab</a>, under Prof. Bryan Pardo. My research is in machine learning, music information retrieval, audio source separation, music structure and similarity, and acoustics. I also <a href="http://soundcloud.com/seeth">write music</a>. Contact me at prem [at] u.northwestern.edu.</p>
    </div>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
        <a href="/">Home</a>
      </li>
      <li class="sidebar-nav-item">
        <a href="/research">Research</a>
      </li>
      <li class="sidebar-nav-item">
        <a href="/music">Music</a>
      </li>

      

      
      
        
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
            <li class="sidebar-nav-item">
              <a href="/notebooks/">Jupyter Notebooks and Lectures</a>
            </li>
          
        
      
        
          
            <li class="sidebar-nav-item">
              <a href="/publications/">Publications</a>
            </li>
          
        
      
        
          
        
      

      <li class="sidebar-nav-item"><a href="/public/cv.pdf" target="_blank">CV</a></li>
    </ul>

  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Cover song identification with 2D Fourier transform sequences</h1>
  <span class="post-date">01 Dec 2016</span>
  <p class="thumbnail-image">
<picture>
    <source srcset="/generated/coversong-600by247-387b86.png" media="(min-width: 40em) and (-webkit-min-device-pixel-ratio: 1.5), (min-width: 40em) and (min-resolution: 144dpi)">
    <source srcset="/generated/coversong-400by165-387b86.png" media="(min-width: 40em)">
    <source srcset="/generated/coversong-375by154-387b86.png" media="(min-width: 30em) and (-webkit-min-device-pixel-ratio: 1.5), (min-width: 30em) and (min-resolution: 144dpi)">
    <source srcset="/generated/coversong-250by103-387b86.png" media="(min-width: 30em)">
    <source srcset="/generated/coversong-525by216-387b86.png" media="(-webkit-min-device-pixel-ratio: 1.5), (min-resolution: 144dpi)">
    <source srcset="/generated/coversong-350by144-387b86.png">
    <img src="/generated/coversong-350by144-387b86.png" class="center-image" data-location="{{location}}" data-active="nil" alt="test" >
  </picture>

</p>

<p>We approach cover song identification using a novel time-series representation of audio based on the 2DFT. The audio is represented as a sequence of magnitude 2D Fourier Transforms (2DFT). This representation is robust to key changes, timbral changes, and small local tempo deviations. We look at cross-similarity between these time-series, and extract a distance measure that is invariant to music structure changes. Our approach is state-of-the-art on a recent cover song dataset, and expands on previous work using the 2DFT for music representation and work on live song recognition.</p>

<!--more-->

<p>I did this work as a research intern at Gracenote in Summer 2016.</p>

<ul>
<li><a href="https://github.com/pseeth/coversong_identification">Source code</a></li>
<li><a href="http://nbviewer.jupyter.org/github/pseeth/coversong_identification/blob/master/presentation/presentation.ipynb">Jupyter notebook</a></li>
<li><a href="/public/papers/seetharaman_rafii_icassp17.pdf">Paper</a></li>
</ul>

</div>

<!--
<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/research/2016/09/16/audealize/">
            Audealize
            <small>16 Sep 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/research/2016/08/01/simultaneous/">
            Source separation and layering structure
            <small>01 Aug 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/music/2016/03/13/sixpieces/">
            six pieces for two voices
            <small>13 Mar 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>
-->

    </div>

  </body>
</html>
